# rag-demo

**Retrieval-Augmented Generation (RAG) demo for PDF documents**

This repo shows a minimal but complete RAG workflow using [LangChain] to:

1. Load one or more user-supplied **PDF** files.
2. Split pages into overlapping text chunks.
3. Embed the chunks with the `all-MiniLM-L6-v2` Sentence-Transformer model.
4. Store embeddings in a local [Chroma] vector store.
5. Answer natural-language questions via an OpenAI LLM + retrieval.

You can interact with it in two ways:

* **Command-line script** (`app.py`)
* **Web UI** built with Streamlit (`streamlit_app.py`)

---

## ‚öôÔ∏è Setup

1. *(Optional)* create and activate a Python ‚â•3.9 virtual environment.
2. Install dependencies:

```bash
pip install -r requirements.txt
```

3. Set your OpenAI API key (needed for the LLM call):

```bash
export OPENAI_API_KEY="<your_openai_key>"
```

---

## üñ•Ô∏è CLI usage (`app.py`)

```bash
python app.py <file1.pdf> [<file2.pdf> ...] --question "Your question here"
```

If `--question/-q` is omitted you will be prompted interactively.

Example:

```bash
python app.py report.pdf notes.pdf -q "Summarise the key findings"
```

Under the hood the script:

1. Loads & splits the PDFs (chunk size 1000, overlap 200)
2. Builds / queries an in-memory Chroma vector store
3. Prints the answer in the terminal

---

## üåê Web UI (`streamlit_app.py`)

Launch with:

```bash
streamlit run streamlit_app.py
```

A browser window will open where you can:

1. **Upload** one or more PDF files (kept in a temporary folder).
2. **Ask** a question about the uploaded documents.
3. See the **answer** generated by the RAG chain.

The vector store is built once per distinct set of uploaded filenames and is cached in the Streamlit session.

---

## üìÅ Project structure

```text
rag-demo/
‚îú‚îÄ‚îÄ app.py              # CLI entry point
‚îú‚îÄ‚îÄ streamlit_app.py    # Streamlit web UI
‚îú‚îÄ‚îÄ requirements.txt    # Python dependencies
‚îú‚îÄ‚îÄ README.md           # This file
‚îî‚îÄ‚îÄ ...
```

---

## üìù Notes & extensions

* The default embedding model and chunk parameters are chosen for speed; adjust as needed for quality.
* Swap `SentenceTransformerEmbeddings` / `OpenAI` for other models (e.g. Cohere, HuggingFace) with minimal code changes.
* You can enable answer source display by uncommenting the section at the bottom of `streamlit_app.py`.

---

Made with ‚ù§Ô∏è and [LangChain], [Chroma], [Sentence-Transformers], and [Streamlit].

[LangChain]: https://github.com/langchain-ai/langchain
[Chroma]: https://www.trychroma.com/


Retrieval-Augmented Generation (RAG) demo built with [LangChain](https://github.com/langchain-ai/langchain).

## Setup

1. (Optional) Create a Python virtual environment.
2. Install dependencies:

```bash
pip install -r requirements.txt
```

3. Export your OpenAI key (required by the demo):

```bash
export OPENAI_API_KEY="<your_openai_key>"
```

## Running the demo

After the setup, simply run:

```bash
python app.py
```

The script will:

1. Load `hello.txt` as the knowledge base.
2. Split the text into chunks and embed them with the `all-MiniLM-L6-v2` sentence-transformer model.
3. Store the embeddings in a local Chroma vector store.
4. Run an OpenAI-powered RetrievalQA chain.
5. Ask the sample question **"What is RAG?"** and print the answer.

Feel free to edit `hello.txt` with your own content or modify `app.py` to experiment with different queries and models.
